{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9320382a-b198-41cf-b3e7-fc6bc9ab6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_21832\\3304139265.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Albumin_and_Globulin_Ratio'].fillna(df['Albumin_and_Globulin_Ratio'].mean(), inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 432 candidates, totalling 1296 fits\n",
      "Tuned Model Accuracy: 0.75\n",
      "Best Parameters: {'colsample_bytree': 1.0, 'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 50, 'subsample': 0.7}\n",
      "Tuned model saved as xgboost_liver_model_tuned.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:13:57] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Liver_data.csv\")\n",
    "\n",
    "# Handle missing values\n",
    "df['Albumin_and_Globulin_Ratio'].fillna(df['Albumin_and_Globulin_Ratio'].mean(), inplace=True)\n",
    "\n",
    "# Encode categorical feature (Gender)\n",
    "le = LabelEncoder()\n",
    "df['Gender'] = le.fit_transform(df['Gender'])  # Male = 1, Female = 0\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop('output', axis=1)\n",
    "y = df['output'] - 1  # Make sure labels are 0 and 1\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Base model\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid,\n",
    "                           cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model and its accuracy\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Tuned Model Accuracy: {accuracy:.2f}\")\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Save the tuned model\n",
    "joblib.dump(best_model, \"xgboost_liver_model_tuned.pkl\")\n",
    "print(\"Tuned model saved as xgboost_liver_model_tuned.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18396859-ed89-47e3-a15f-ea7b21acfc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: ['Age', 'Gender', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase', 'Alamine_Aminotransferase', 'Aspartate_Aminotransferase', 'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio', 'output']\n",
      "Using 'output' as target column.\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 0.603448275862069\n",
      "Confusion Matrix:\n",
      " [[25  8]\n",
      " [15 10]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68        33\n",
      "           1       0.56      0.40      0.47        25\n",
      "\n",
      "    accuracy                           0.60        58\n",
      "   macro avg       0.59      0.58      0.58        58\n",
      "weighted avg       0.60      0.60      0.59        58\n",
      "\n",
      "\n",
      "=== AdaBoost ===\n",
      "Accuracy: 0.6206896551724138\n",
      "Confusion Matrix:\n",
      " [[21 12]\n",
      " [10 15]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66        33\n",
      "           1       0.56      0.60      0.58        25\n",
      "\n",
      "    accuracy                           0.62        58\n",
      "   macro avg       0.62      0.62      0.62        58\n",
      "weighted avg       0.62      0.62      0.62        58\n",
      "\n",
      "\n",
      "=== Gradient Boosting ===\n",
      "Accuracy: 0.6551724137931034\n",
      "Confusion Matrix:\n",
      " [[24  9]\n",
      " [11 14]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71        33\n",
      "           1       0.61      0.56      0.58        25\n",
      "\n",
      "    accuracy                           0.66        58\n",
      "   macro avg       0.65      0.64      0.64        58\n",
      "weighted avg       0.65      0.66      0.65        58\n",
      "\n",
      "\n",
      "=== XGBoost ===\n",
      "Accuracy: 0.603448275862069\n",
      "Confusion Matrix:\n",
      " [[24  9]\n",
      " [14 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.73      0.68        33\n",
      "           1       0.55      0.44      0.49        25\n",
      "\n",
      "    accuracy                           0.60        58\n",
      "   macro avg       0.59      0.58      0.58        58\n",
      "weighted avg       0.60      0.60      0.60        58\n",
      "\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "\n",
      "=== Tuned XGBoost ===\n",
      "Accuracy: 0.6206896551724138\n",
      "Confusion Matrix:\n",
      " [[23 10]\n",
      " [12 13]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.70      0.68        33\n",
      "           1       0.57      0.52      0.54        25\n",
      "\n",
      "    accuracy                           0.62        58\n",
      "   macro avg       0.61      0.61      0.61        58\n",
      "weighted avg       0.62      0.62      0.62        58\n",
      "\n",
      "Best Parameters from RandomizedSearchCV: {'subsample': 0.6, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.2, 'colsample_bytree': 0.6}\n",
      "\n",
      "=== Model Accuracy Comparison ===\n",
      "Random Forest:       0.60\n",
      "AdaBoost:            0.62\n",
      "Gradient Boosting:   0.66\n",
      "XGBoost:             0.60\n",
      "Tuned XGBoost:       0.62\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('Liver_data.csv')  # Replace with your actual dataset path\n",
    "\n",
    "# Print columns to verify\n",
    "print(\"Columns in dataset:\", data.columns.tolist())\n",
    "\n",
    "# Define target column\n",
    "target_col = 'output'\n",
    "print(f\"Using '{target_col}' as target column.\")\n",
    "\n",
    "# Drop rows where target value is missing\n",
    "data.dropna(subset=[target_col], inplace=True)\n",
    "\n",
    "# Encode categorical column 'Gender' (if present) to numeric values\n",
    "if 'Gender' in data.columns:\n",
    "    le_gender = LabelEncoder()\n",
    "    data['Gender'] = le_gender.fit_transform(data['Gender'])\n",
    "\n",
    "# Now fill missing values using median (all columns should be numeric now)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Remove outliers using the Interquartile Range (IQR) method\n",
    "Q1 = data_imputed.quantile(0.25)\n",
    "Q3 = data_imputed.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data_clean = data_imputed[~((data_imputed < (Q1 - 1.5 * IQR)) | (data_imputed > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Separate features and target\n",
    "X = data_clean.drop([target_col], axis=1)\n",
    "y = data_clean[target_col]\n",
    "\n",
    "# Remap target values to binary classes [0, 1]\n",
    "# Assuming in the original dataset 'output' has values 1 and 2, with 2 as the positive class.\n",
    "y = np.where(y == 2, 1, 0)\n",
    "\n",
    "# Feature Selection using SelectKBest (keeping all features)\n",
    "selector = SelectKBest(score_func=f_classif, k='all')\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "X = pd.DataFrame(X_selected, columns=X.columns[selector.get_support(indices=True)])\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Initialize models\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "ada = AdaBoostClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "# Train models\n",
    "rf.fit(X_train, y_train)\n",
    "ada.fit(X_train, y_train)\n",
    "gb.fit(X_train, y_train)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "y_pred_xgb = xgb.predict(X_test)\n",
    "\n",
    "# Define a helper function to print evaluation metrics\n",
    "def print_results(name, y_true, y_pred):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# Evaluation of each model\n",
    "print_results(\"Random Forest\", y_test, y_pred_rf)\n",
    "print_results(\"AdaBoost\", y_test, y_pred_ada)\n",
    "print_results(\"Gradient Boosting\", y_test, y_pred_gb)\n",
    "print_results(\"XGBoost\", y_test, y_pred_xgb)\n",
    "\n",
    "# XGBoost Hyperparameter Tuning with RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    estimator=XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=20,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train, y_train)\n",
    "best_xgb = xgb_random.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "print_results(\"Tuned XGBoost\", y_test, y_pred_best_xgb)\n",
    "print(\"Best Parameters from RandomizedSearchCV:\", xgb_random.best_params_)\n",
    "\n",
    "# Summary of Model Accuracies\n",
    "print(\"\\n=== Model Accuracy Comparison ===\")\n",
    "print(f\"Random Forest:       {accuracy_score(y_test, y_pred_rf):.2f}\")\n",
    "print(f\"AdaBoost:            {accuracy_score(y_test, y_pred_ada):.2f}\")\n",
    "print(f\"Gradient Boosting:   {accuracy_score(y_test, y_pred_gb):.2f}\")\n",
    "print(f\"XGBoost:             {accuracy_score(y_test, y_pred_xgb):.2f}\")\n",
    "print(f\"Tuned XGBoost:       {accuracy_score(y_test, y_pred_best_xgb):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ec554-e770-4e86-92d2-a978971e86a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
